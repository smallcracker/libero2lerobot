# -*- coding: utf-8 -*-
"""
Liberoç»Ÿä¸€æ•°æ®è½¬æ¢å™¨

æ”¯æŒè‡ªåŠ¨è¯†åˆ«RLDSå’ŒHDF5æ ¼å¼ï¼Œè‡ªåŠ¨è§£ææˆLeRobotDataSet v2.1æ ¼å¼
"""

import argparse
import json
import logging
import math
import shutil
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple

import cv2
import h5py
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm
import jsonlines

# æ£€æŸ¥ä¾èµ–
try:
    import tensorflow_datasets as tfds
    import tensorflow as tf

    HAS_TF = True
except ImportError:
    HAS_TF = False
    logging.warning("tensorflow_datasetsæœªå®‰è£…ï¼ŒRLDSæ”¯æŒå°†è¢«ç¦ç”¨")


# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [%(processName)s:%(process)d] [%(filename)s:%(lineno)d] %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# v2.1 ç‰ˆæœ¬å¸¸é‡
V21 = "v2.1"
DEFAULT_CHUNK_SIZE = 1000
LEGACY_DATA_PATH_TEMPLATE = (
    "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet"
)
LEGACY_VIDEO_PATH_TEMPLATE = (
    "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4"
)
LEGACY_EPISODES_PATH = "meta/episodes.jsonl"
LEGACY_EPISODES_STATS_PATH = "meta/episodes_stats.jsonl"
LEGACY_TASKS_PATH = "meta/tasks.jsonl"


def quat_to_euler(quat: np.ndarray) -> np.ndarray:
    """
    å°†å››å…ƒæ•°è½¬æ¢ä¸ºæ¬§æ‹‰è§’ (roll, pitch, yaw)

    Args:
        quat: å››å…ƒæ•°æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (..., 4)ï¼Œæ ¼å¼ä¸º [x, y, z, w] æˆ– [w, x, y, z]
              æ ¹æ®LIBEROçš„æƒ¯ä¾‹ï¼Œå‡è®¾æ ¼å¼ä¸º [x, y, z, w]

    Returns:
        æ¬§æ‹‰è§’æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (..., 3)ï¼Œæ ¼å¼ä¸º [roll, pitch, yaw]ï¼Œå•ä½ä¸ºå¼§åº¦
    """
    # å‡è®¾å››å…ƒæ•°æ ¼å¼ä¸º [x, y, z, w]
    x, y, z, w = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]

    # Roll (x-axis rotation)
    sinr_cosp = 2.0 * (w * x + y * z)
    cosr_cosp = 1.0 - 2.0 * (x * x + y * y)
    roll = np.arctan2(sinr_cosp, cosr_cosp)

    # Pitch (y-axis rotation)
    sinp = 2.0 * (w * y - z * x)
    # å¤„ç†ä¸‡å‘é”æƒ…å†µ
    sinp = np.clip(sinp, -1.0, 1.0)
    pitch = np.arcsin(sinp)

    # Yaw (z-axis rotation)
    siny_cosp = 2.0 * (w * z + x * y)
    cosy_cosp = 1.0 - 2.0 * (y * y + z * z)
    yaw = np.arctan2(siny_cosp, cosy_cosp)

    return np.stack([roll, pitch, yaw], axis=-1)


# ä»»åŠ¡åç§°å¸¸é‡
TASK_NAME = (
    "Stack the red block on the blue block, then stack the green block on the red block"
)

logger.info("Libero RLDS/HDF5è½¬æ¢å™¨åˆå§‹åŒ– - v2.1æ ¼å¼")


class DatasetFormatDetector:
    """æ•°æ®é›†æ ¼å¼æ£€æµ‹å™¨"""

    @staticmethod
    def detect_format(data_path: Union[str, Path]) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼

        Args:
            data_path: æ•°æ®é›†è·¯å¾„

        Returns:
            str: 'rlds' æˆ– 'hdf5'
        """
        data_path = Path(data_path)

        # æ£€æŸ¥HDF5æ ¼å¼ï¼šæŸ¥æ‰¾.hdf5æˆ–.h5æ–‡ä»¶
        hdf5_files = list(data_path.rglob("*.hdf5")) + list(data_path.rglob("*.h5"))

        # æ£€æŸ¥RLDSæ ¼å¼ï¼šæŸ¥æ‰¾tfrecordæ–‡ä»¶æˆ–dataset_info.json
        rlds_indicators = (
            list(data_path.rglob("*.tfrecord*"))
            + list(data_path.rglob("dataset_info.json"))
            + list(data_path.rglob("features.json"))
        )

        if hdf5_files and not rlds_indicators:
            logger.info(f"æ£€æµ‹åˆ°HDF5æ ¼å¼ï¼Œæ‰¾åˆ°{len(hdf5_files)}ä¸ªHDF5æ–‡ä»¶")
            return "hdf5"
        elif rlds_indicators and not hdf5_files:
            logger.info(
                f"æ£€æµ‹åˆ°RLDSæ ¼å¼ï¼Œæ‰¾åˆ°ç›¸å…³æ–‡ä»¶ï¼š{[f.name for f in rlds_indicators[:3]]}"
            )
            return "rlds"
        elif hdf5_files and rlds_indicators:
            logger.warning("åŒæ—¶å‘ç°HDF5å’ŒRLDSæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨HDF5æ ¼å¼")
            return "hdf5"
        else:
            raise ValueError(f"æ— æ³•æ£€æµ‹æ•°æ®æ ¼å¼ï¼š{data_path}")


class LeRobotDatasetV21Writer:
    """LeRobot v2.1 æ ¼å¼æ•°æ®é›†å†™å…¥å™¨"""

    def __init__(
        self,
        repo_id: str,
        root: Path,
        robot_type: str = "panda",
        fps: int = 20,
        features: Dict[str, Dict[str, Any]] = None,
        use_videos: bool = True,
        chunks_size: int = DEFAULT_CHUNK_SIZE,
    ):
        self.repo_id = repo_id
        self.root = Path(root)
        self.robot_type = robot_type
        self.fps = fps
        self.features = features or {}
        self.use_videos = use_videos
        self.chunks_size = chunks_size

        # çŠ¶æ€è·Ÿè¸ª
        self.episode_index = 0
        self.current_episode_frames: List[Dict[str, Any]] = []
        self.episodes_metadata: List[Dict[str, Any]] = []
        self.tasks: Dict[str, int] = {}  # task_name -> task_index
        self.total_frames = 0

        # è§†é¢‘å†™å…¥å™¨
        self.video_writers: Dict[str, cv2.VideoWriter] = {}
        self.video_keys = [
            k for k, v in self.features.items() if v.get("dtype") == "video"
        ]

        # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å™¨ - ç”¨äºè®¡ç®—å…¨å±€ç»Ÿè®¡
        self.stats_collector: Dict[str, List[np.ndarray]] = {}
        for key in self.features:
            if self.features[key].get("dtype") != "video":
                self.stats_collector[key] = []

        # åˆ›å»ºç›®å½•ç»“æ„
        self._init_directories()

    def _init_directories(self):
        """åˆå§‹åŒ–ç›®å½•ç»“æ„"""
        self.root.mkdir(parents=True, exist_ok=True)
        (self.root / "meta").mkdir(exist_ok=True)
        (self.root / "data").mkdir(exist_ok=True)
        if self.use_videos:
            (self.root / "videos").mkdir(exist_ok=True)

    def _get_or_create_task_index(self, task_name: str) -> int:
        """è·å–æˆ–åˆ›å»ºä»»åŠ¡ç´¢å¼•"""
        if task_name not in self.tasks:
            self.tasks[task_name] = len(self.tasks)
        return self.tasks[task_name]

    def add_frame(self, frame_data: Dict[str, Any]):
        """æ·»åŠ ä¸€å¸§æ•°æ®"""
        self.current_episode_frames.append(frame_data.copy())

    def save_episode(self):
        """ä¿å­˜å½“å‰episode"""
        if not self.current_episode_frames:
            return

        episode_length = len(self.current_episode_frames)
        episode_chunk = self.episode_index // self.chunks_size

        # è·å–ä»»åŠ¡ä¿¡æ¯
        task_name = self.current_episode_frames[0].get("task", TASK_NAME)
        task_index = self._get_or_create_task_index(task_name)

        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        self._collect_stats()

        # å‡†å¤‡parquetæ•°æ®
        parquet_data = self._prepare_parquet_data(task_index)

        # ä¿å­˜parquetæ–‡ä»¶
        self._save_parquet(episode_chunk, parquet_data)

        # ä¿å­˜è§†é¢‘æ–‡ä»¶ï¼ˆå¦‚æœä½¿ç”¨è§†é¢‘ï¼‰
        if self.use_videos:
            self._save_videos(episode_chunk)

        # è®°å½•episodeå…ƒæ•°æ®
        self.episodes_metadata.append(
            {
                "episode_index": self.episode_index,
                "length": episode_length,
                "tasks": [task_name],
            }
        )

        self.total_frames += episode_length
        self.episode_index += 1
        self.current_episode_frames = []

        logger.debug(f"ä¿å­˜episode {self.episode_index - 1}, é•¿åº¦: {episode_length}")

    def _collect_stats(self):
        """æ”¶é›†å½“å‰episodeçš„ç»Ÿè®¡æ•°æ®"""
        for key in self.stats_collector:
            values = []
            for frame in self.current_episode_frames:
                if key in frame:
                    value = frame[key]
                    if isinstance(value, np.ndarray):
                        values.append(value.astype(np.float64))
            if values:
                # å°†æ•´ä¸ªepisodeçš„æ•°æ®å †å èµ·æ¥
                stacked = np.stack(values, axis=0)
                self.stats_collector[key].append(stacked)

    def _prepare_parquet_data(self, task_index: int) -> Dict[str, List]:
        """å‡†å¤‡parquetæ ¼å¼çš„æ•°æ®"""
        data = {
            "frame_index": [],
            "timestamp": [],
            "episode_index": [],
            "index": [],
            "task_index": [],
        }

        # åˆå§‹åŒ–ç‰¹å¾åˆ—
        for key in self.features:
            if self.features[key].get("dtype") == "video":
                # è§†é¢‘ç‰¹å¾ä¸å­˜å‚¨åœ¨parquetä¸­
                continue
            data[key] = []

        for frame_idx, frame in enumerate(self.current_episode_frames):
            data["frame_index"].append(frame_idx)
            data["timestamp"].append(frame_idx / self.fps)
            data["episode_index"].append(self.episode_index)
            data["index"].append(self.total_frames + frame_idx)
            data["task_index"].append(task_index)

            for key in self.features:
                if self.features[key].get("dtype") == "video":
                    continue
                if key in frame:
                    value = frame[key]
                    if isinstance(value, np.ndarray):
                        data[key].append(value.tolist())
                    else:
                        data[key].append(value)

        return data

    def _save_parquet(self, episode_chunk: int, data: Dict[str, List]):
        """ä¿å­˜parquetæ–‡ä»¶"""
        parquet_path = self.root / LEGACY_DATA_PATH_TEMPLATE.format(
            episode_chunk=episode_chunk,
            episode_index=self.episode_index,
        )
        parquet_path.parent.mkdir(parents=True, exist_ok=True)

        df = pd.DataFrame(data)
        table = pa.Table.from_pandas(df)
        pq.write_table(table, parquet_path)

    def _save_videos(self, episode_chunk: int):
        """ä¿å­˜è§†é¢‘æ–‡ä»¶"""
        for video_key in self.video_keys:
            video_path = self.root / LEGACY_VIDEO_PATH_TEMPLATE.format(
                episode_chunk=episode_chunk,
                video_key=video_key,
                episode_index=self.episode_index,
            )
            video_path.parent.mkdir(parents=True, exist_ok=True)

            # è·å–å›¾åƒå°ºå¯¸
            shape = self.features[video_key].get("shape", (256, 256, 3))
            height, width = shape[0], shape[1]

            # ä½¿ç”¨ffmpegå‹å¥½çš„ç¼–ç å™¨
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            writer = cv2.VideoWriter(str(video_path), fourcc, self.fps, (width, height))

            for frame in self.current_episode_frames:
                if video_key in frame:
                    img = frame[video_key]
                    if img.shape[2] == 3:
                        # RGB -> BGR for OpenCV
                        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                    writer.write(img)

            writer.release()

    def finalize(self):
        """å®Œæˆæ•°æ®é›†å†™å…¥ï¼Œä¿å­˜æ‰€æœ‰å…ƒæ•°æ®"""
        logger.info("æ­£åœ¨ä¿å­˜v2.1æ ¼å¼çš„å…ƒæ•°æ®...")

        # ä¿å­˜ info.json
        self._save_info()

        # ä¿å­˜ tasks.jsonl
        self._save_tasks()

        # ä¿å­˜ episodes.jsonl å’Œ episodes_stats.jsonl
        self._save_episodes_metadata()

        # ä¿å­˜ stats.json
        self._save_stats()

        logger.info(
            f"æ•°æ®é›†ä¿å­˜å®Œæˆ: {self.episode_index} episodes, {self.total_frames} frames"
        )

    def _save_info(self):
        """ä¿å­˜ info.json"""
        total_episodes = self.episode_index

        info = {
            "codebase_version": V21,
            "robot_type": self.robot_type,
            "fps": self.fps,
            "features": self.features,
            "data_path": LEGACY_DATA_PATH_TEMPLATE,
            "video_path": LEGACY_VIDEO_PATH_TEMPLATE
            if self.use_videos and self.video_keys
            else None,
            "total_episodes": total_episodes,
            "total_frames": self.total_frames,
            "total_chunks": math.ceil(total_episodes / self.chunks_size)
            if total_episodes > 0
            else 0,
            "total_videos": total_episodes * len(self.video_keys)
            if self.video_keys
            else 0,
            "chunks_size": self.chunks_size,
            "total_tasks": len(self.tasks),
        }

        info_path = self.root / "meta" / "info.json"
        with open(info_path, "w") as f:
            json.dump(info, f, indent=4)

    def _save_tasks(self):
        """ä¿å­˜ tasks.jsonl"""
        tasks_path = self.root / LEGACY_TASKS_PATH
        tasks_path.parent.mkdir(parents=True, exist_ok=True)

        # æŒ‰ task_index æ’åº
        sorted_tasks = sorted(self.tasks.items(), key=lambda x: x[1])

        with jsonlines.open(tasks_path, mode="w") as writer:
            for task_name, task_index in sorted_tasks:
                writer.write(
                    {
                        "task_index": task_index,
                        "task": task_name,
                    }
                )

    def _save_episodes_metadata(self):
        """ä¿å­˜ episodes.jsonl å’Œ episodes_stats.jsonl"""
        episodes_path = self.root / LEGACY_EPISODES_PATH
        stats_path = self.root / LEGACY_EPISODES_STATS_PATH
        episodes_path.parent.mkdir(parents=True, exist_ok=True)

        with jsonlines.open(episodes_path, mode="w") as episodes_writer:
            with jsonlines.open(stats_path, mode="w") as stats_writer:
                for metadata in self.episodes_metadata:
                    episodes_writer.write(metadata)
                    # ç®€å•çš„ç»Ÿè®¡ä¿¡æ¯
                    stats_writer.write(
                        {
                            "episode_index": metadata["episode_index"],
                            "stats": {},
                        }
                    )

    def _save_stats(self):
        """ä¿å­˜ stats.json - å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        for key, data_list in self.stats_collector.items():
            if not data_list:
                continue

            # å°†æ‰€æœ‰episodeçš„æ•°æ®åˆå¹¶
            all_data = np.concatenate(data_list, axis=0)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            feature_stats = {
                "min": all_data.min(axis=0).tolist(),
                "max": all_data.max(axis=0).tolist(),
                "mean": all_data.mean(axis=0).tolist(),
                "std": all_data.std(axis=0).tolist(),
            }

            # å¦‚æœæ˜¯æ ‡é‡ï¼Œå±•å¼€ä¸ºå•ä¸ªå€¼
            if (
                isinstance(feature_stats["min"], list)
                and len(feature_stats["min"]) == 1
            ):
                for stat_key in feature_stats:
                    if isinstance(feature_stats[stat_key], list):
                        feature_stats[stat_key] = feature_stats[stat_key]

            stats[key] = feature_stats

        stats_path = self.root / "meta" / "stats.json"
        with open(stats_path, "w") as f:
            json.dump(stats, f, indent=4)

        logger.info(f"å·²ä¿å­˜ç»Ÿè®¡ä¿¡æ¯: {list(stats.keys())}")


class HDF5Processor:
    """HDF5æ•°æ®å¤„ç†å™¨"""

    def __init__(
        self, image_size: Tuple[int, int] = (256, 256), use_videos: bool = False
    ):
        self.image_size = image_size  # (height, width)
        self.use_videos = use_videos

    def get_default_features(
        self, use_videos: bool = True
    ) -> Dict[str, Dict[str, Any]]:
        """è·å–Liberoæ•°æ®é›†çš„é»˜è®¤ç‰¹å¾é…ç½®"""
        image_dtype = "video" if use_videos else "image"

        return {
            "observation.images.front": {
                "dtype": image_dtype,
                "shape": (*self.image_size, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.images.wrist": {
                "dtype": image_dtype,
                "shape": (*self.image_size, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.state": {
                "dtype": "float32",
                "shape": (8,),
                "names": [f"state_{i}" for i in range(8)],
            },
            "action": {
                "dtype": "float32",
                "shape": (7,),
                "names": [f"action_{i}" for i in range(7)],
            },
        }

    def process_episode(
        self, episode_path: Path, dataset: LeRobotDatasetV21Writer, task_name: str
    ) -> bool:
        """å¤„ç†å•ä¸ªepisodeæ•°æ®"""
        try:
            with h5py.File(episode_path, "r") as file:
                logger.info(f"HDF5æ–‡ä»¶é”®: {list(file.keys())}")

                if "data" in file:
                    return self._process_libero_demo_format(file, dataset, episode_path)
                else:
                    logger.warning(f"æœªè¯†åˆ«çš„HDF5æ ¼å¼: {episode_path}")
                    return False

        except (FileNotFoundError, OSError, KeyError) as e:
            logger.error(f"è·³è¿‡ {episode_path}: {str(e)}")
            return False

    def _process_libero_demo_format(
        self,
        file: h5py.File,
        dataset: LeRobotDatasetV21Writer,
        file_path: Optional[Path] = None,
    ) -> bool:
        """å¤„ç†æ–°çš„Libero demoæ ¼å¼ï¼šdata/demo_N/..."""
        data_group = file["data"]

        demo_keys = [k for k in data_group.keys() if k.startswith("demo_")]
        demo_keys.sort(key=lambda x: int(x.split("_")[1]))

        for demo_key in demo_keys:
            demo_group = data_group[demo_key]
            logger.info(f"å¤„ç† {demo_key}")

            demo_task_str = TASK_NAME

            actions = np.array(demo_group["actions"])
            obs_group = demo_group["obs"]

            if "eef_pos" not in obs_group:
                raise KeyError("æœªæ‰¾åˆ°æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®æ•°æ® (eef_pos)")
            if "eef_quat" not in obs_group:
                raise KeyError("æœªæ‰¾åˆ°æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€æ•°æ® (eef_quat)")
            if "gripper_pos" not in obs_group:
                raise KeyError("æœªæ‰¾åˆ°å¤¹çˆªçŠ¶æ€æ•°æ® (gripper_pos)")

            eef_pos = np.array(obs_group["eef_pos"])
            eef_quat = np.array(obs_group["eef_quat"])
            gripper_pos = np.array(obs_group["gripper_pos"])

            eef_euler = quat_to_euler(eef_quat)
            joint_states = np.concatenate([eef_pos, eef_euler, gripper_pos], axis=-1)
            logger.info(f"çŠ¶æ€å‘é‡å½¢çŠ¶: {joint_states.shape}")

            if "agentview_rgb" in obs_group and "eye_in_hand_rgb" in obs_group:
                agentview_rgb = np.array(obs_group["agentview_rgb"])
                eye_in_hand_rgb = np.array(obs_group["eye_in_hand_rgb"])
            elif "table_cam" in obs_group and "wrist_cam" in obs_group:
                agentview_rgb = np.array(obs_group["table_cam"])
                eye_in_hand_rgb = np.array(obs_group["wrist_cam"])
            else:
                raise KeyError("æœªæ‰¾åˆ°å›¾åƒæ•°æ®")

            num_frames = min(
                len(actions),
                len(joint_states),
                len(agentview_rgb),
                len(eye_in_hand_rgb),
            )

            logger.info(
                f"Demo {demo_key}: actions={len(actions)}, states={len(joint_states)}, front={len(agentview_rgb)}, wrist={len(eye_in_hand_rgb)}"
            )

            for i in tqdm(range(num_frames), desc=f"å¤„ç† {demo_key}", leave=False):
                front_img = cv2.resize(
                    agentview_rgb[i], (self.image_size[1], self.image_size[0])
                )
                wrist_img = cv2.resize(
                    eye_in_hand_rgb[i], (self.image_size[1], self.image_size[0])
                )

                frame_data = {
                    "action": actions[i].astype(np.float32),
                    "observation.state": joint_states[i].astype(np.float32),
                    "observation.images.front": front_img,
                    "observation.images.wrist": wrist_img,
                    "task": demo_task_str,
                }

                dataset.add_frame(frame_data)

            dataset.save_episode()

        return True


class RLDSProcessor:
    """RLDSæ•°æ®å¤„ç†å™¨"""

    def __init__(self):
        if not HAS_TF:
            raise ImportError("tensorflow_datasetsæ˜¯RLDSå¤„ç†æ‰€å¿…éœ€çš„")
        self.image_size = (256, 256)
        self.use_videos = False

    def get_default_features(
        self, use_videos: bool = True
    ) -> Dict[str, Dict[str, Any]]:
        """è·å–Liberoæ•°æ®é›†çš„é»˜è®¤ç‰¹å¾é…ç½®"""
        image_dtype = "video" if use_videos else "image"

        return {
            "observation.images.front": {
                "dtype": image_dtype,
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.images.wrist": {
                "dtype": image_dtype,
                "shape": (256, 256, 3),
                "names": ["height", "width", "channel"],
            },
            "observation.state": {
                "dtype": "float32",
                "shape": (8,),
                "names": [f"state_{i}" for i in range(8)],
            },
            "action": {
                "dtype": "float32",
                "shape": (7,),
                "names": [f"action_{i}" for i in range(7)],
            },
        }

    def process_dataset(
        self, dataset: LeRobotDatasetV21Writer, data_source: Union[str, Path]
    ):
        """å¤„ç†RLDSæ•°æ®é›†"""
        raw_dataset_names = [
            "libero_10_no_noops",
            "libero_goal_no_noops",
            "libero_object_no_noops",
            "libero_spatial_no_noops",
        ]

        episode_idx = 0

        for raw_dataset_name in raw_dataset_names:
            logger.info(f"å¤„ç†RLDSæ•°æ®é›†: {raw_dataset_name}")

            try:
                raw_dataset = tfds.load(
                    raw_dataset_name, data_dir=data_source, split="train", try_gcs=False
                )

                for episode in raw_dataset:
                    logger.info(f"å¤„ç†episode {episode_idx + 1}")

                    steps_list = list(episode["steps"].as_numpy_iterator())
                    task_str = f"episode_{episode_idx}"

                    if steps_list and "language_instruction" in steps_list[0]:
                        task_str = steps_list[0]["language_instruction"].decode()

                    for step_idx, step in enumerate(steps_list):
                        frame_data = {
                            "observation.images.front": step["observation"]["image"],
                            "observation.images.wrist": step["observation"][
                                "wrist_image"
                            ],
                            "observation.state": step["observation"]["state"].astype(
                                np.float32
                            ),
                            "action": step["action"].astype(np.float32),
                            "task": task_str,
                        }
                        dataset.add_frame(frame_data)

                    dataset.save_episode()
                    episode_idx += 1

            except Exception as e:
                logger.warning(f"å¤„ç†æ•°æ®é›† {raw_dataset_name} æ—¶å‡ºé”™: {e}")
                continue


class UnifiedConverter:
    """ç»Ÿä¸€è½¬æ¢å™¨ç±» - ç”Ÿæˆv2.1æ ¼å¼"""

    def __init__(self):
        self.detector = DatasetFormatDetector()
        logger.info("è½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ - ç”Ÿæˆv2.1æ ¼å¼æ•°æ®é›†")

    def convert_dataset(
        self,
        data_dir: Union[str, Path],
        repo_id: str,
        output_dir: Optional[Union[str, Path]] = None,
        push_to_hub: bool = False,
        use_videos: bool = True,
        robot_type: str = "panda",
        fps: int = 20,
        task_name: str = "default_task",
        hub_config: Optional[Dict[str, Any]] = None,
        clean_existing: bool = False,
        **kwargs,
    ) -> LeRobotDatasetV21Writer:
        """
        ç»Ÿä¸€è½¬æ¢æ¥å£ - ç”Ÿæˆv2.1æ ¼å¼

        Returns:
            LeRobotDatasetV21Writer: æ•°æ®é›†å†™å…¥å™¨
        """
        data_path = Path(data_dir)

        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        format_type = self.detector.detect_format(data_path)
        logger.info(f"æ£€æµ‹åˆ°æ•°æ®æ ¼å¼: {format_type}")

        # æ ¹æ®æ ¼å¼é€‰æ‹©å¤„ç†å™¨å’Œç‰¹å¾
        if format_type == "hdf5":
            processor = HDF5Processor()
            features = processor.get_default_features(use_videos)
        else:
            processor = RLDSProcessor()
            features = processor.get_default_features(use_videos)

        # è®¾ç½®è¾“å‡ºè·¯å¾„
        if output_dir is None:
            lerobot_root = Path("~/.cache/huggingface/lerobot/").expanduser()
        else:
            lerobot_root = Path(output_dir).expanduser()

        os.environ["HF_LEROBOT_HOME"] = str(lerobot_root)
        lerobot_dataset_dir = lerobot_root / repo_id

        # æ£€æŸ¥ç°æœ‰æ•°æ®é›†
        if clean_existing and lerobot_dataset_dir.exists():
            logger.info(f"æ¸…ç†ç°æœ‰æ•°æ®é›†: {lerobot_dataset_dir}")
            shutil.rmtree(lerobot_dataset_dir)

        lerobot_root.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºv2.1æ ¼å¼çš„æ•°æ®é›†å†™å…¥å™¨
        logger.info(f"åˆ›å»ºLeRobot v2.1æ•°æ®é›†: {repo_id}")
        logger.info(f"æœºå™¨äººç±»å‹: {robot_type}, å¸§ç‡: {fps}, ä½¿ç”¨è§†é¢‘: {use_videos}")

        dataset = LeRobotDatasetV21Writer(
            repo_id=repo_id,
            root=lerobot_dataset_dir,
            robot_type=robot_type,
            fps=fps,
            features=features,
            use_videos=use_videos,
        )

        # å¤„ç†æ•°æ®
        if format_type == "hdf5":
            self._process_hdf5_data(processor, dataset, data_path, TASK_NAME)
        else:
            if isinstance(processor, RLDSProcessor):
                processor.process_dataset(dataset, data_path)
            else:
                raise TypeError("RLDSæ ¼å¼éœ€è¦RLDSProcessorå®ä¾‹")

        # å®Œæˆå†™å…¥ï¼Œä¿å­˜æ‰€æœ‰å…ƒæ•°æ®
        dataset.finalize()

        logger.info("âœ… v2.1æ ¼å¼æ•°æ®é›†è½¬æ¢å®Œæˆ!")
        return dataset

    def _process_hdf5_data(
        self,
        processor: HDF5Processor,
        dataset: LeRobotDatasetV21Writer,
        data_path: Path,
        task_name: str,
    ):
        """å•çº¿ç¨‹å¤„ç†HDF5æ•°æ®"""
        # æŸ¥æ‰¾æ‰€æœ‰episode
        episodes = []
        for ep_dir in data_path.iterdir():
            if ep_dir.is_dir():
                ep_path = ep_dir / "data" / "trajectory.hdf5"
                if ep_path.exists():
                    episodes.append(ep_path)

        if not episodes:
            episodes = list(data_path.rglob("*.hdf5")) + list(data_path.rglob("*.h5"))

        logger.info(f"æ‰¾åˆ° {len(episodes)} ä¸ªepisodeæ–‡ä»¶")

        for ep_path in tqdm(episodes, desc="å¤„ç†Episodes"):
            processor.process_episode(ep_path, dataset, TASK_NAME)
            logger.info(f"å¤„ç†å®Œæˆ: {ep_path.name}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Liberoç»Ÿä¸€æ•°æ®è½¬æ¢å™¨ - ç”ŸæˆLeRobot v2.1æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("--data-dir", type=str, required=True, help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--repo-id", type=str, required=True, help="æ•°æ®é›†ä»“åº“ID")
    parser.add_argument("--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--push-to-hub", action="store_true", help="æ¨é€åˆ°Hub")
    parser.add_argument("--private", action="store_true", help="åˆ›å»ºç§æœ‰æ•°æ®é›†")
    parser.add_argument("--use-videos", action="store_true", help="ä½¿ç”¨è§†é¢‘æ ¼å¼")
    parser.add_argument("--robot-type", type=str, default="panda", help="æœºå™¨äººç±»å‹")
    parser.add_argument("--fps", type=int, default=20, help="å¸§ç‡")
    parser.add_argument(
        "--task-name", type=str, default="default_task", help="ä»»åŠ¡åç§°"
    )
    parser.add_argument("--clean-existing", action="store_true", help="æ¸…ç†ç°æœ‰æ•°æ®é›†")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†æ—¥å¿—")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    if not Path(args.data_dir).exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return 1

    if "/" not in args.repo_id:
        logger.error(f"repo_idæ ¼å¼é”™è¯¯: {args.repo_id}")
        return 1

    logger.info("ğŸ“‹ è½¬æ¢é…ç½® (v2.1æ ¼å¼):")
    logger.info(f"  æ•°æ®æº: {args.data_dir}")
    logger.info(f"  ä»“åº“ID: {args.repo_id}")
    logger.info(f"  ä½¿ç”¨è§†é¢‘: {args.use_videos}")

    if args.dry_run:
        logger.info("âœ… è¯•è¿è¡Œå®Œæˆï¼Œå‚æ•°éªŒè¯é€šè¿‡")
        return 0

    try:
        converter = UnifiedConverter()

        dataset = converter.convert_dataset(
            data_dir=args.data_dir,
            repo_id=args.repo_id,
            output_dir=args.output_dir,
            push_to_hub=args.push_to_hub,
            use_videos=args.use_videos,
            robot_type=args.robot_type,
            fps=args.fps,
            task_name=TASK_NAME,
            clean_existing=args.clean_existing,
        )

        logger.info("âœ… è½¬æ¢å®Œæˆ!")
        return 0

    except Exception as e:
        logger.error(f"è½¬æ¢å¤±è´¥: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys

    sys.exit(main())
