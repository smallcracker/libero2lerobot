#!/usr/bin/env python3
"""
å¿«é€Ÿåˆ†æLiberoæ•°æ®é›†çš„parquetæ–‡ä»¶
"""

import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import io
import json

def analyze_libero_dataset(dataset_path):
    dataset_path = Path(dataset_path).expanduser()

    # è¯»å–info.json
    with open(dataset_path / "meta" / "info.json", 'r') as f:
        info = json.load(f)

    print("=" * 60)
    print("Liberoæ•°æ®é›†å¿«é€Ÿåˆ†æ")
    print("=" * 60)

    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ€»Episodes: {info['total_episodes']}")
    print(f"  æ€»å¸§æ•°: {info['total_frames']}")
    print(f"  FPS: {info['fps']}")
    print(f"  æœºå™¨äººç±»å‹: {info['robot_type']}")

    # è®¡ç®—å¹³å‡episodeé•¿åº¦
    avg_length = info['total_frames'] / info['total_episodes']
    print(f"  å¹³å‡Episodeé•¿åº¦: {avg_length:.1f} å¸§")
    print(f"  ä¼°è®¡æ€»æ—¶é•¿: {info['total_frames'] / info['fps']:.1f} ç§’")

    # å›¾åƒä¿¡æ¯
    print(f"\nğŸ“· å›¾åƒä¿¡æ¯:")
    front_shape = info['features']['observation.images.front']['shape']
    wrist_shape = info['features']['observation.images.wrist']['shape']
    print(f"  Frontç›¸æœº: {front_shape[0]}x{front_shape[1]} ({front_shape[2]} é€šé“)")
    print(f"  Wristç›¸æœº: {wrist_shape[0]}x{wrist_shape[1]} ({wrist_shape[2]} é€šé“)")

    # Actionå’ŒStateç»´åº¦
    action_dim = info['features']['action']['shape'][0]
    state_dim = info['features']['observation.state']['shape'][0]
    print(f"  Actionç»´åº¦: {action_dim}")
    print(f"  Stateç»´åº¦: {state_dim}")

    # åˆ†æparquetæ–‡ä»¶
    data_dir = dataset_path / "data"
    parquet_files = list(data_dir.rglob("*.parquet"))

    print(f"\nğŸ“ Parquetæ–‡ä»¶:")
    print(f"  æ–‡ä»¶æ•°é‡: {len(parquet_files)}")

    # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶æ¥åˆ†æç»“æ„
    first_file = parquet_files[0]
    df = pd.read_parquet(first_file)

    print(f"\nğŸ” æ ·æœ¬æ•°æ®ç»“æ„ (æ¥è‡ª {first_file.name}):")
    print(f"  è¡Œæ•°: {len(df)}")
    print(f"  åˆ—: {list(df.columns)}")

    # åˆ†æepisodeåˆ†å¸ƒ
    episodes = df['episode_index'].unique()
    print(f"  EpisodesèŒƒå›´: {min(episodes)} - {max(episodes)}")
    print(f"  Episodeæ•°é‡: {len(episodes)}")

    # åˆ†æç¬¬ä¸€ä¸ªepisodeçš„è¯¦ç»†ä¿¡æ¯
    first_episode = min(episodes)
    first_ep_data = df[df['episode_index'] == first_episode]

    if len(first_ep_data) > 1:
        timestamps = first_ep_data['timestamp'].tolist()
        timestamps = [ts[0] if isinstance(ts, (list, np.ndarray)) else ts for ts in timestamps]

        time_diffs = np.diff(timestamps)
        avg_dt = np.mean(time_diffs)
        std_dt = np.std(time_diffs)

        print(f"\nğŸ¯ Episode {first_episode} è¯¦ç»†ä¿¡æ¯:")
        print(f"  å¸§æ•°: {len(first_ep_data)}")
        print(f"  æ—¶é—´é—´éš”: {avg_dt:.4f}s Â± {std_dt:.4f}s")
        print(f"  ä¼°è®¡FPS: {1/avg_dt:.1f}" if avg_dt > 0 else "  FPS: æ— æ³•è®¡ç®—")
        print(f"  æŒç»­æ—¶é—´: {timestamps[-1] - timestamps[0]:.2f}s")

        # åˆ†æå›¾åƒæ•°æ®
        try:
            front_img_data = first_ep_data['observation.images.front'].iloc[0]
            if isinstance(front_img_data, dict) and 'bytes' in front_img_data:
                img = Image.open(io.BytesIO(front_img_data['bytes']))
                print(f"  Frontå›¾åƒå®é™…å°ºå¯¸: {img.size} (æ¨¡å¼: {img.mode})")
        except Exception as e:
            print(f"  å›¾åƒåˆ†æå¤±è´¥: {e}")

    # åˆ†ææ–‡ä»¶å¤§å°
    total_size = sum(f.stat().st_size for f in parquet_files) / (1024 * 1024 * 1024)
    print(f"\nğŸ’¾ å­˜å‚¨ä¿¡æ¯:")
    print(f"  Parquetæ–‡ä»¶æ€»å¤§å°: {total_size:.2f} GB")
    print(f"  å¹³å‡æ–‡ä»¶å¤§å°: {total_size/len(parquet_files):.2f} GB")

    # åˆ†ææ‰€æœ‰æ–‡ä»¶çš„episodeåˆ†å¸ƒ
    all_episodes = []
    episode_lengths = {}

    for parquet_file in parquet_files[:5]:  # åªåˆ†æå‰5ä¸ªæ–‡ä»¶ä»¥èŠ‚çœæ—¶é—´
        try:
            df = pd.read_parquet(parquet_file)
            file_episodes = df['episode_index'].unique()

            for ep in file_episodes:
                ep_length = len(df[df['episode_index'] == ep])
                episode_lengths[int(ep)] = ep_length
                all_episodes.append(int(ep))
        except Exception as e:
            print(f"  âŒ è¯»å– {parquet_file.name} å¤±è´¥: {e}")

    if episode_lengths:
        lengths = list(episode_lengths.values())
        print(f"\nğŸ“ˆ Episodeé•¿åº¦ç»Ÿè®¡:")
        print(f"  é•¿åº¦èŒƒå›´: {min(lengths)} - {max(lengths)} å¸§")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} å¸§")
        print(f"  ä¸­ä½æ•°é•¿åº¦: {np.median(lengths):.1f} å¸§")
        print(f"  æ ‡å‡†å·®: {np.std(lengths):.1f} å¸§")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python quick_explore.py <dataset_path>")
        sys.exit(1)

    analyze_libero_dataset(sys.argv[1])