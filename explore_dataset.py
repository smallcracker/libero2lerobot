#!/usr/bin/env python3
"""
Liberoæ•°æ®é›†æ¢ç´¢å·¥å…·
åˆ†æLeRobotæ ¼å¼çš„Liberoæ•°æ®é›†ï¼Œæä¾›è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯å’Œå¯è§†åŒ–
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import io
import argparse
from typing import Dict, List, Tuple, Any
from collections import defaultdict

try:
    from lerobot.datasets.lerobot_dataset import LeRobotDataset
except ImportError:
    print("é”™è¯¯: è¯·å®‰è£…LeRobot: pip install lerobot")
    sys.exit(1)


def analyze_dataset_basic_info(dataset_path: str) -> Dict[str, Any]:
    """åˆ†ææ•°æ®é›†åŸºæœ¬ä¿¡æ¯"""
    dataset_path = Path(dataset_path).expanduser()

    if not dataset_path.exists():
        return {"error": f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}"}

    # è¯»å–info.json
    info_file = dataset_path / "meta" / "info.json"
    if not info_file.exists():
        return {"error": "æ‰¾ä¸åˆ°meta/info.jsonæ–‡ä»¶"}

    with open(info_file, "r", encoding="utf-8") as f:
        info = json.load(f)

    # å°è¯•åŠ è½½LeRobotæ•°æ®é›†
    try:
        dataset = LeRobotDataset(repo_id="", root=str(dataset_path))
        total_frames = len(dataset)

        # è·å–episodeç»Ÿè®¡
        episode_indices = dataset.episode_data_index["episode_index"].tolist()
        total_episodes = len(set(episode_indices))

        # è®¡ç®—å¹³å‡episodeé•¿åº¦
        episode_lengths = []
        for ep_idx in set(episode_indices):
            ep_mask = episode_indices == ep_idx
            episode_lengths.append(np.sum(ep_mask))

        avg_length = np.mean(episode_lengths) if episode_lengths else 0
        min_length = np.min(episode_lengths) if episode_lengths else 0
        max_length = np.max(episode_lengths) if episode_lengths else 0

        return {
            "dataset_path": str(dataset_path),
            "total_episodes": total_episodes,
            "total_frames": total_frames,
            "avg_episode_length": avg_length,
            "min_episode_length": min_length,
            "max_episode_length": max_length,
            "fps": info.get("fps", "unknown"),
            "features": info.get("features", {}),
            "splits": info.get("splits", {}),
            "dataset_size_mb": info.get("data_files_size_in_mb", "unknown"),
        }

    except Exception as e:
        return {
            "error": f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}",
            "dataset_path": str(dataset_path),
            "info": info,
        }


def analyze_demo_details(dataset_path: str, max_demos: int = 10) -> Dict[str, Any]:
    """è¯¦ç»†åˆ†ædemoä¿¡æ¯"""
    dataset_path = Path(dataset_path).expanduser()

    try:
        dataset = LeRobotDataset(repo_id="", root=str(dataset_path))

        demo_info = []
        episode_indices = dataset.episode_data_index["episode_index"].tolist()
        unique_episodes = sorted(set(episode_indices))

        # é™åˆ¶åˆ†æçš„demoæ•°é‡
        episodes_to_analyze = unique_episodes[: min(max_demos, len(unique_episodes))]

        for ep_idx in episodes_to_analyze:
            # è·å–è¯¥episodeçš„æ‰€æœ‰å¸§ç´¢å¼•
            ep_mask = np.array(episode_indices) == ep_idx
            frame_indices = np.where(ep_mask)[0]

            if len(frame_indices) == 0:
                continue

            # è·å–ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§çš„æ—¶é—´æˆ³æ¥è®¡ç®—dt
            first_frame_idx = frame_indices[0]
            last_frame_idx = frame_indices[-1]

            # è·å–æ—¶é—´æˆ³ä¿¡æ¯
            timestamps = dataset.frames["timestamp"][frame_indices]

            if len(timestamps) > 1:
                dt_list = np.diff(timestamps.flatten())
                avg_dt = np.mean(dt_list)
                std_dt = np.std(dt_list)
            else:
                avg_dt = 0
                std_dt = 0

            # åˆ†æå›¾åƒå°ºå¯¸
            try:
                # è·å–ç¬¬ä¸€å¸§çš„å›¾åƒ
                front_img_data = dataset.frames["observation.images.front"][
                    first_frame_idx
                ]
                if isinstance(front_img_data, dict) and "bytes" in front_img_data:
                    img = Image.open(io.BytesIO(front_img_data["bytes"]))
                    image_size = img.size  # (width, height)
                    image_mode = img.mode
                else:
                    # å¦‚æœæ˜¯ç›´æ¥å­˜å‚¨çš„numpyæ•°ç»„
                    image_size = (
                        front_img_data.shape[:2][::-1]
                        if hasattr(front_img_data, "shape")
                        else "unknown"
                    )
                    image_mode = "array"
            except:
                image_size = "unknown"
                image_mode = "unknown"

            # è·å–actionå’Œstateçš„ç»´åº¦
            try:
                action_sample = dataset.frames["action"][first_frame_idx]
                action_dim = (
                    action_sample.shape[-1]
                    if hasattr(action_sample, "shape")
                    else len(action_sample)
                )
            except:
                action_dim = "unknown"

            try:
                state_sample = dataset.frames["observation.state"][first_frame_idx]
                state_dim = (
                    state_sample.shape[-1]
                    if hasattr(state_sample, "shape")
                    else len(state_sample)
                )
            except:
                state_dim = "unknown"

            demo_info.append(
                {
                    "demo_id": int(ep_idx),
                    "num_frames": len(frame_indices),
                    "avg_dt": float(avg_dt),
                    "std_dt": float(std_dt),
                    "estimated_fps": 1.0 / avg_dt if avg_dt > 0 else 0,
                    "image_size": image_size,
                    "image_mode": image_mode,
                    "action_dim": action_dim,
                    "state_dim": state_dim,
                    "duration_seconds": float(timestamps[-1][0] - timestamps[0][0])
                    if len(timestamps) > 0
                    else 0,
                }
            )

        return {
            "analyzed_demos": len(demo_info),
            "total_demos": len(unique_episodes),
            "demo_details": demo_info,
        }

    except Exception as e:
        return {"error": f"åˆ†ædemoè¯¦æƒ…å¤±è´¥: {str(e)}"}


def analyze_parquet_files(dataset_path: str) -> Dict[str, Any]:
    """åˆ†æparquetæ–‡ä»¶ç»“æ„"""
    dataset_path = Path(dataset_path).expanduser()
    data_dir = dataset_path / "data"

    if not data_dir.exists():
        return {"error": "æ‰¾ä¸åˆ°dataç›®å½•"}

    parquet_files = list(data_dir.rglob("*.parquet"))

    if not parquet_files:
        return {"error": "æ‰¾ä¸åˆ°parquetæ–‡ä»¶"}

    file_info = []
    total_rows = 0

    for parquet_file in sorted(parquet_files):
        try:
            df = pd.read_parquet(parquet_file)

            # åˆ†ææ¯åˆ—çš„æ•°æ®ç±»å‹å’Œå¤§å°
            column_info = {}
            for col in df.columns:
                if (
                    col == "observation.images.front"
                    or col == "observation.images.wrist"
                ):
                    # å›¾åƒåˆ—çš„ç‰¹æ®Šåˆ†æ
                    sample = df[col].iloc[0] if len(df) > 0 else None
                    if isinstance(sample, dict) and "bytes" in sample:
                        column_info[col] = {
                            "dtype": str(df[col].dtype),
                            "sample_size_bytes": len(sample["bytes"]),
                            "format": "image_binary",
                        }
                    else:
                        column_info[col] = {
                            "dtype": str(df[col].dtype),
                            "shape": str(df[col].iloc[0].shape)
                            if hasattr(df[col].iloc[0], "shape")
                            else "unknown",
                            "format": "array",
                        }
                else:
                    column_info[col] = {
                        "dtype": str(df[col].dtype),
                        "shape": str(df[col].iloc[0].shape)
                        if len(df) > 0 and hasattr(df[col].iloc[0], "shape")
                        else "scalar",
                    }

            file_info.append(
                {
                    "file_path": str(parquet_file.relative_to(dataset_path)),
                    "rows": len(df),
                    "columns": list(df.columns),
                    "file_size_mb": parquet_file.stat().st_size / (1024 * 1024),
                    "column_details": column_info,
                }
            )

            total_rows += len(df)

        except Exception as e:
            file_info.append(
                {
                    "file_path": str(parquet_file.relative_to(dataset_path)),
                    "error": str(e),
                }
            )

    return {
        "total_parquet_files": len(parquet_files),
        "total_rows": total_rows,
        "files": file_info,
    }


def print_summary(dataset_path: str):
    """æ‰“å°æ•°æ®é›†æ‘˜è¦ä¿¡æ¯"""
    print("=" * 60)
    print("Liberoæ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # åŸºæœ¬ä¿¡æ¯
    basic_info = analyze_dataset_basic_info(dataset_path)
    if "error" in basic_info:
        print(f"âŒ é”™è¯¯: {basic_info['error']}")
        return

    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ•°æ®é›†è·¯å¾„: {basic_info['dataset_path']}")
    print(f"  æ€»Episodes: {basic_info['total_episodes']}")
    print(f"  æ€»å¸§æ•°: {basic_info['total_frames']}")
    print(f"  å¹³å‡Episodeé•¿åº¦: {basic_info['avg_episode_length']:.1f} å¸§")
    print(
        f"  Episodeé•¿åº¦èŒƒå›´: {basic_info['min_episode_length']} - {basic_info['max_episode_length']} å¸§"
    )
    print(f"  FPS: {basic_info['fps']}")
    print(f"  æ•°æ®é›†å¤§å°: {basic_info['dataset_size_mb']} MB")

    # Demoè¯¦æƒ…
    print(f"\nğŸ” Demoè¯¦æƒ… (å‰5ä¸ª):")
    demo_details = analyze_demo_details(dataset_path, max_demos=5)

    if "error" not in demo_details:
        for demo in demo_details["demo_details"]:
            print(f"  Demo {demo['demo_id']}:")
            print(f"    å¸§æ•°: {demo['num_frames']}")
            print(f"    å¹³å‡dt: {demo['avg_dt']:.4f}s Â± {demo['std_dt']:.4f}s")
            print(f"    ä¼°è®¡FPS: {demo['estimated_fps']:.1f}")
            print(f"    æŒç»­æ—¶é—´: {demo['duration_seconds']:.2f}s")
            print(f"    å›¾åƒå°ºå¯¸: {demo['image_size']}")
            print(f"    Actionç»´åº¦: {demo['action_dim']}")
            print(f"    Stateç»´åº¦: {demo['state_dim']}")
            print()

    # Parquetæ–‡ä»¶åˆ†æ
    print(f"ğŸ“ Parquetæ–‡ä»¶ç»“æ„:")
    parquet_info = analyze_parquet_files(dataset_path)

    if "error" not in parquet_info:
        print(f"  æ€»æ–‡ä»¶æ•°: {parquet_info['total_parquet_files']}")
        print(f"  æ€»è¡Œæ•°: {parquet_info['total_rows']}")
        print(f"  æ–‡ä»¶è¯¦æƒ…:")

        for file in parquet_info["files"]:
            if "error" in file:
                print(f"    âŒ {file['file_path']}: {file['error']}")
            else:
                print(f"    ğŸ“„ {file['file_path']}:")
                print(f"      è¡Œæ•°: {file['rows']}")
                print(f"      æ–‡ä»¶å¤§å°: {file['file_size_mb']:.2f} MB")
                print(f"      åˆ—: {', '.join(file['columns'])}")


def main():
    parser = argparse.ArgumentParser(description="æ¢ç´¢Liberoæ•°æ®é›†")
    parser.add_argument("dataset_path", help="LeRobotæ•°æ®é›†è·¯å¾„")
    parser.add_argument("--max-demos", type=int, default=10, help="åˆ†æçš„æœ€å¤§demoæ•°é‡")
    parser.add_argument("--output", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # æ‰“å°æ‘˜è¦
    print_summary(args.dataset_path)

    # å¦‚æœéœ€è¦è¾“å‡ºåˆ°æ–‡ä»¶
    if args.output:
        analysis_result = {
            "basic_info": analyze_dataset_basic_info(args.dataset_path),
            "demo_details": analyze_demo_details(args.dataset_path, args.max_demos),
            "parquet_analysis": analyze_parquet_files(args.dataset_path),
        }

        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
